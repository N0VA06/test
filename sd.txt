from optimum.onnxruntime import ORTModelForFeatureExtraction
import torch
from transformers import AutoTokenizer
from fastapi import FastAPI, HTTPException, Request, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.exceptions import RequestValidationError
from pydantic import BaseModel, Field, validator
from typing import List, Dict, Union, Optional, Any
import numpy as np
import uvicorn
import logging
from logging.handlers import RotatingFileHandler
import time
import traceback
from contextlib import asynccontextmanager
import os
import sys
import json
from functools import lru_cache
import threading
import queue
import asyncio

# Set up advanced logging
log_dir = "logs"
os.makedirs(log_dir, exist_ok=True)

# Create a custom formatter
formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Set up file handler with rotation
file_handler = RotatingFileHandler(
    os.path.join(log_dir, "api.log"),
    maxBytes=10485760,  # 10MB
    backupCount=5
)
file_handler.setFormatter(formatter)

# Set up console handler
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)

# Configure root logger
logging.basicConfig(
    level=logging.INFO,
    handlers=[file_handler, console_handler]
)

logger = logging.getLogger(__name__)

# Constants
MODEL_ID = "BAAI/bge-base-en-v1.5"
REVISION = "refs/pr/13"  # Use the specific PR you mentioned
MAX_RETRY_ATTEMPTS = 3
RETRY_DELAY = 2  # seconds
MAX_BATCH_SIZE = 100  # Maximum number of texts to process at once
REQUEST_TIMEOUT = 60  # seconds
MAX_TEXT_LENGTH = 5000  # Maximum length of each text

# Global variables
models = {}
tokenizers = {}
model_load_lock = threading.Lock()
embedding_queue = queue.Queue()
is_processing = False

# Lifecycle management
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    try:
        await load_primary_model()
    except Exception as e:
        logger.error(f"Failed to load primary model at startup: {e}")
        logger.error(traceback.format_exc())
        # We'll continue without a pre-loaded model and try again on first request
    
    yield  # Application runs here
    
    # Shutdown
    logger.info("Shutting down application, releasing resources...")
    global models, tokenizers
    models = {}
    tokenizers = {}
    torch.cuda.empty_cache() if torch.cuda.is_available() else None

# Initialize FastAPI app with lifespan
app = FastAPI(
    title="BGE Embedding API",
    description="API for generating embeddings using BGE model with ONNX optimization",
    version="1.0.1",
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allow all methods
    allow_headers=["*"],  # Allow all headers
)

# Define request and response models with enhanced validation
class EmbeddingRequest(BaseModel):
    texts: List[str] = Field(..., min_items=1, max_items=MAX_BATCH_SIZE)
    normalize: bool = True
    instruction: Optional[str] = None  # Optional instruction to prepend to queries
    
    @validator('texts')
    def validate_texts(cls, v):
        # Check for empty texts
        if any(not text.strip() for text in v):
            raise ValueError("Empty texts are not allowed")
        
        # Check for texts that are too long
        long_texts = [i for i, text in enumerate(v) if len(text) > MAX_TEXT_LENGTH]
        if long_texts:
            raise ValueError(f"Texts at indices {long_texts} exceed the maximum allowed length of {MAX_TEXT_LENGTH} characters")
        
        return v
    
class ErrorDetail(BaseModel):
    type: str
    message: str
    traceback: Optional[str] = None

class EmbeddingResponse(BaseModel):
    embeddings: List[List[float]]
    dimensions: int
    model: str
    processing_time_ms: float
    using_fallback: bool = False
    
class ModelStatus(BaseModel):
    status: str
    model: str
    is_primary: bool
    device: str
    uptime_seconds: Optional[float] = None
    loaded_at: Optional[str] = None
    
class SystemStatus(BaseModel):
    status: str
    models: Dict[str, ModelStatus]
    gpu_available: bool
    queue_size: int
    version: str

# Custom exception handler for validation errors
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    errors = exc.errors()
    error_messages = []
    
    for error in errors:
        error_location = " -> ".join(str(loc) for loc in error["loc"])
        error_messages.append(f"{error_location}: {error['msg']}")
    
    return JSONResponse(
        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
        content={"detail": error_messages}
    )

# Global exception handler
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"Unhandled exception: {str(exc)}")
    logger.error(traceback.format_exc())
    
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "detail": "Internal server error",
            "error": str(exc),
            "type": type(exc).__name__
        }
    )

# Enhanced model loading with ONNX optimization
async def load_primary_model():
    with model_load_lock:
        global models, tokenizers
        if "primary" in models and "primary" in tokenizers:
            return models["primary"], tokenizers["primary"]
        
        logger.info(f"Loading primary model {MODEL_ID} with ONNX optimization...")
        start_time = time.time()
        
        try:
            # Get token from environment if available
            hf_token = os.getenv("HUGGINGFACE_TOKEN")
            token_kwargs = {"token": hf_token} if hf_token else {}
            
            # Load tokenizer
            tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, revision=REVISION, **token_kwargs)
            
            # Load ONNX model
            device = "cuda" if torch.cuda.is_available() else "cpu"
            model_ort = ORTModelForFeatureExtraction.from_pretrained(
                MODEL_ID, 
                revision=REVISION,
                file_name="model.onnx",
                **token_kwargs
            )
            
            if device == "cuda" and hasattr(model_ort, "to"):
                model_ort = model_ort.to(torch.device("cuda"))
                logger.info("Primary model loaded on GPU")
            else:
                logger.info("Primary model loaded on CPU")
            
            models["primary"] = {
                "model": model_ort,
                "device": device,
                "loaded_at": time.time(),
                "is_primary": True
            }
            
            tokenizers["primary"] = tokenizer
            
            logger.info(f"Primary model loaded in {time.time() - start_time:.2f} seconds")
            return models["primary"], tokenizers["primary"]
        except Exception as e:
            logger.error(f"Failed to load primary model: {e}")
            logger.error(traceback.format_exc())
            raise

async def load_fallback_model():
    with model_load_lock:
        global models, tokenizers
        if "fallback" in models and "fallback" in tokenizers:
            return models["fallback"], tokenizers["fallback"]
        
        logger.info("Loading fallback model...")
        start_time = time.time()
        
        try:
            # Get token from environment if available
            hf_token = os.getenv("HUGGINGFACE_TOKEN")
            token_kwargs = {"token": hf_token} if hf_token else {}
            
            # Always load fallback on CPU for reliability - use standard model instead of ONNX
            from transformers import AutoModel
            tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, revision=REVISION, **token_kwargs)
            model = AutoModel.from_pretrained(MODEL_ID, revision=REVISION, **token_kwargs)
            
            models["fallback"] = {
                "model": model,
                "device": "cpu",
                "loaded_at": time.time(),
                "is_primary": False,
                "is_onnx": False  # Flag to indicate this is not ONNX
            }
            
            tokenizers["fallback"] = tokenizer
            
            logger.info(f"Fallback model loaded in {time.time() - start_time:.2f} seconds")
            return models["fallback"], tokenizers["fallback"]
        except Exception as e:
            logger.error(f"Failed to load fallback model: {e}")
            logger.error(traceback.format_exc())
            raise

# Helper function to get a working model with fallback
async def get_working_model(attempt=0):
    try:
        # Try to get or load primary model
        model_data, tokenizer = await load_primary_model()
        return model_data, tokenizer, False
    except Exception as e:
        logger.warning(f"Error with primary model (attempt {attempt+1}/{MAX_RETRY_ATTEMPTS}): {e}")
        
        # If we've tried enough times with the primary model, use fallback
        if attempt >= MAX_RETRY_ATTEMPTS - 1:
            logger.warning("Maximum retry attempts reached, trying fallback model")
            try:
                model_data, tokenizer = await load_fallback_model()
                return model_data, tokenizer, True
            except Exception as fallback_error:
                logger.error(f"Fallback model also failed: {fallback_error}")
                raise HTTPException(
                    status_code=503,
                    detail="All embedding models are currently unavailable"
                )
        
        # Wait before retrying
        await asyncio.sleep(RETRY_DELAY)
        return await get_working_model(attempt + 1)

# Mean pooling function
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]  # First element contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

# Normalize embeddings
def normalize_embeddings(embeddings):
    return embeddings / embeddings.norm(p=2, dim=1, keepdim=True)

# Enhanced embedding endpoint with retries and error handling
@app.post("/embeddings", response_model=EmbeddingResponse)
async def get_embeddings(request: EmbeddingRequest):
    if not request.texts:
        raise HTTPException(status_code=400, detail="No text provided")
    
    # Check if batch size is within limits
    if len(request.texts) > MAX_BATCH_SIZE:
        raise HTTPException(
            status_code=400, 
            detail=f"Batch size exceeds maximum allowed ({len(request.texts)} > {MAX_BATCH_SIZE})"
        )
    
    try:
        start_time = time.time()
        
        # Get a working model (with fallback if necessary)
        model_data, tokenizer, using_fallback = await get_working_model()
        model = model_data["model"]
        
        # Apply instruction if provided (for query-to-passage retrieval)
        texts = request.texts
        if request.instruction:
            texts = [f"{request.instruction} {text}" for text in texts]
        
        # Generate embeddings with retry logic
        for attempt in range(MAX_RETRY_ATTEMPTS):
            try:
                # Tokenize the texts
                encoded_input = tokenizer(
                    texts, 
                    padding=True, 
                    truncation=True, 
                    return_tensors='pt'
                )
                
                # Move to GPU if available
                if model_data["device"] == "cuda":
                    encoded_input = {k: v.to("cuda") for k, v in encoded_input.items()}
                
                # Get model output
                with torch.no_grad():
                    if model_data.get("is_onnx", True):  # ONNX model
                        model_output = model(**encoded_input)
                        # Get embeddings using mean pooling
                        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
                    else:  # Standard model
                        model_output = model(**encoded_input)
                        # Get embeddings using mean pooling
                        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
                
                # Normalize embeddings if requested
                if request.normalize:
                    sentence_embeddings = normalize_embeddings(sentence_embeddings)
                
                # Convert to numpy for consistency
                embeddings = sentence_embeddings.cpu().numpy()
                break
            except Exception as e:
                logger.warning(f"Error during embedding generation (attempt {attempt+1}/{MAX_RETRY_ATTEMPTS}): {e}")
                if attempt == MAX_RETRY_ATTEMPTS - 1:
                    # Last attempt failed, try fallback if not already using it
                    if not using_fallback:
                        logger.warning("Switching to fallback model for this request")
                        model_data, tokenizer = await load_fallback_model()
                        model = model_data["model"]
                        using_fallback = True
                        
                        # Continue to next iteration to try with fallback
                        continue
                    else:
                        # Already using fallback and still failing
                        raise
                await asyncio.sleep(RETRY_DELAY)
        
        # Convert numpy arrays to lists for JSON serialization
        embeddings_list = embeddings.tolist()
        
        processing_time = (time.time() - start_time) * 1000  # Convert to milliseconds
        
        return EmbeddingResponse(
            embeddings=embeddings_list,
            dimensions=len(embeddings_list[0]),
            model=MODEL_ID,
            processing_time_ms=processing_time,
            using_fallback=using_fallback
        )
    except Exception as e:
        logger.error(f"Error generating embeddings: {str(e)}")
        logger.error(traceback.format_exc())
        
        error_detail = ErrorDetail(
            type=type(e).__name__,
            message=str(e),
            traceback=traceback.format_exc() if os.getenv("DEBUG") == "true" else None
        )
        
        raise HTTPException(
            status_code=500, 
            detail=f"Error generating embeddings: {error_detail.dict(exclude_none=True)}"
        )

# Enhanced model info endpoint
@app.get("/model-info")
async def model_info():
    # Make sure at least one model is loaded
    try:
        model_data = None
        tokenizer = None
        
        if "primary" in models:
            model_data = models["primary"]
            tokenizer = tokenizers["primary"]
        elif "fallback" in models:
            model_data = models["fallback"]
            tokenizer = tokenizers["fallback"]
        else:
            model_data, tokenizer = await load_primary_model()
        
        model = model_data["model"]
        
        # Get the embedding dimension
        # For ONNX models, we need to infer this from a test sentence
        try:
            test_input = tokenizer(["test"], padding=True, truncation=True, return_tensors='pt')
            if model_data["device"] == "cuda":
                test_input = {k: v.to("cuda") for k, v in test_input.items()}
            
            with torch.no_grad():
                test_output = model(**test_input)
                
            sentence_embedding = mean_pooling(test_output, test_input['attention_mask'])
            embedding_dim = sentence_embedding.size(-1)
        except Exception as e:
            logger.warning(f"Error determining embedding dimension: {e}")
            embedding_dim = "unknown"
        
        return {
            "model_name": MODEL_ID,
            "revision": REVISION,
            "dimensions": embedding_dim,
            "provider": "BAAI",
            "model_url": f"https://huggingface.co/{MODEL_ID}",
            "embedding_type": "dense",
            "recommended_normalize": True,
            "max_sequence_length": getattr(tokenizer, "model_max_length", 512),
            "device": model_data["device"],
            "is_onnx": model_data.get("is_onnx", True),
            "is_primary": model_data.get("is_primary", True)
        }
    except Exception as e:
        logger.error(f"Error getting model info: {str(e)}")
        raise HTTPException(
            status_code=500, 
            detail=f"Error getting model info: {str(e)}"
        )

# Add health check endpoint
@app.get("/health", response_model=SystemStatus)
async def health_check():
    global models, tokenizers, embedding_queue
    
    model_statuses = {}
    system_status = "healthy"
    
    # Check all loaded models
    for model_type, model_data in models.items():
        uptime = None
        loaded_at = None
        
        if "loaded_at" in model_data:
            uptime = time.time() - model_data["loaded_at"]
            loaded_at = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(model_data["loaded_at"]))
        
        model_statuses[model_type] = ModelStatus(
            status="loaded",
            model=MODEL_ID,
            is_primary=model_data.get("is_primary", False),
            device=model_data.get("device", "unknown"),
            uptime_seconds=uptime,
            loaded_at=loaded_at
        )

    # If no models are loaded, system is degraded
    if not models:
        system_status = "degraded"
    
    return SystemStatus(
        status=system_status,
        models=model_statuses,
        gpu_available=torch.cuda.is_available(),
        queue_size=embedding_queue.qsize(),
        version=app.version
    )

# Add diagnostics endpoint
@app.get("/diagnostics")
async def diagnostics():
    if not os.getenv("DEBUG") == "true":
        raise HTTPException(
            status_code=403,
            detail="Diagnostics endpoint is only available in debug mode"
        )
    
    # Get system info
    system_info = {
        "os": sys.platform,
        "python_version": sys.version,
        "cpu_count": os.cpu_count(),
        "memory_info": {}
    }
    
    # GPU info if available
    gpu_info = {}
    if torch.cuda.is_available():
        gpu_info["device_count"] = torch.cuda.device_count()
        gpu_info["current_device"] = torch.cuda.current_device()
        gpu_info["device_name"] = torch.cuda.get_device_name(0)
        try:
            gpu_info["memory_allocated"] = torch.cuda.memory_allocated(0)
            gpu_info["memory_reserved"] = torch.cuda.memory_reserved(0)
        except:
            gpu_info["memory_stats"] = "Not available"
    
    # Model info
    model_info = {}
    for model_type, model_data in models.items():
        model_obj = model_data.get("model")
        if model_obj:
            model_info[model_type] = {
                "device": model_data.get("device", "unknown"),
                "is_onnx": model_data.get("is_onnx", True),
                "is_primary": model_data.get("is_primary", False),
                "loaded_at": time.strftime('%Y-%m-%d %H:%M:%S', 
                                          time.localtime(model_data.get("loaded_at", 0))),
            }
    
    return {
        "system": system_info,
        "gpu": gpu_info,
        "models": model_info,
        "queue_size": embedding_queue.qsize(),
        "environment_variables": {k: v for k, v in os.environ.items() 
                                 if k.startswith(("MODEL_", "MAX_", "RETRY_", "DEBUG", "HUGGINGFACE_"))}
    }

# Add reset endpoint for recovery scenarios
@app.post("/admin/reset-models")
async def reset_models(request: Request):
    # Simple API key auth for admin endpoints
    api_key = request.headers.get("x-api-key")
    admin_key = os.getenv("ADMIN_API_KEY")
    
    if not admin_key or api_key != admin_key:
        raise HTTPException(status_code=401, detail="Unauthorized")
    
    with model_load_lock:
        global models, tokenizers
        logger.warning("Manual model reset triggered via admin endpoint")
        
        # Release CUDA memory if applicable
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Clear models dictionary
        models = {}
        tokenizers = {}
        
        # Try to load primary model again
        try:
            await load_primary_model()
            return {"status": "success", "message": "Models reset successfully"}
        except Exception as e:
            logger.error(f"Failed to reload models after reset: {e}")
            return {
                "status": "partial_success", 
                "message": "Models were reset but failed to reload automatically",
                "error": str(e)
            }

if __name__ == "__main__":
    # Get port from environment or use default
    port = int(os.getenv("PORT", 8000))
    
    # Set debug mode from environment
    debug_mode = os.getenv("DEBUG", "false").lower() == "true"
    
    if debug_mode:
        logger.setLevel(logging.DEBUG)
        logger.debug("Debug mode enabled")
    
    logger.info(f"Starting server on port {port}")
    uvicorn.run("main:app", host="0.0.0.0", port=port, reload=debug_mode)

import os
import sys
import argparse
import requests
import json
import time
from typing import List, Dict, Any, Optional
import logging
from pathlib import Path
import PyPDF2
import uuid
from tqdm import tqdm
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams, PointStruct

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Constants
EMBEDDING_API_URL = "http://localhost:8000/embeddings"
CHUNK_SIZE = 3000
CHUNK_OVERLAP = 50
EMBEDDING_DIMENSION = 768  # BGE base model dimension
QDRANT_HOST = "localhost"
QDRANT_PORT = 6333
COLLECTION_NAME = "documents"

class PDFProcessor:
    def __init__(self, embedding_api_url: str, qdrant_client: QdrantClient):
        self.embedding_api_url = embedding_api_url
        self.qdrant_client = qdrant_client
        
    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:
        """Extract text from PDF and split into pages with metadata."""
        logger.info(f"Extracting text from {pdf_path}")
        
        result = []
        file_name = Path(pdf_path).name
        
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                total_pages = len(pdf_reader.pages)
                
                for page_num, page in enumerate(pdf_reader.pages):
                    text = page.extract_text()
                    if text.strip():  # Only include non-empty pages
                        result.append({
                            "text": text,
                            "metadata": {
                                "source": pdf_path,
                                "page": page_num + 1,
                                "tot_pages": total_pages,
                                "file_name": file_name
                            }
                        })
                        
            logger.info(f"Extracted {len(result)} pages from {file_name}")
            return result
        except Exception as e:
            logger.error(f"Error extracting text from {pdf_path}: {str(e)}")
            return []
    
    def chunk_text(self, pages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Split pages into smaller chunks with preserved metadata."""
        chunks = []
        
        for page in pages:
            text = page["text"]
            metadata = page["metadata"]
            
            # Simple text chunking by characters with overlap
            text_length = len(text)
            
            if text_length <= CHUNK_SIZE:
                # If text is smaller than chunk size, keep it as is
                chunks.append({
                    "text": text,
                    "metadata": metadata
                })
            else:
                # Split text into overlapping chunks
                for i in range(0, text_length, CHUNK_SIZE - CHUNK_OVERLAP):
                    chunk_text = text[i:min(i + CHUNK_SIZE, text_length)]
                    if len(chunk_text.strip()) > 0:
                        # Create a copy of metadata for each chunk
                        chunk_metadata = metadata.copy()
                        # Add chunk information to metadata
                        chunk_metadata["chunk_id"] = len(chunks)
                        
                        chunks.append({
                            "text": chunk_text,
                            "metadata": chunk_metadata
                        })
        
        logger.info(f"Created {len(chunks)} chunks from {len(pages)} pages")
        return chunks
    
    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings using the BGE embedding API."""
        try:
            # Prepare the request payload
            payload = {
                "texts": texts,
                "normalize": True
            }
            
            # Make the API request
            response = requests.post(
                self.embedding_api_url,
                json=payload,
                headers={"Content-Type": "application/json"}
            )
            
            # Check if the request was successful
            if response.status_code == 200:
                embeddings = response.json().get("embeddings", [])
                logger.info(f"Generated {len(embeddings)} embeddings")
                return embeddings
            else:
                logger.error(f"Error generating embeddings: {response.text}")
                return []
                
        except Exception as e:
            logger.error(f"Exception during embedding generation: {str(e)}")
            return []
    
    def upload_to_qdrant(self, chunks: List[Dict[str, Any]], embeddings: List[List[float]]) -> bool:
        """Upload chunks and their embeddings to Qdrant."""
        try:
            if len(chunks) != len(embeddings):
                logger.error(f"Number of chunks ({len(chunks)}) doesn't match number of embeddings ({len(embeddings)})")
                return False
            
            # Prepare points for Qdrant
            points = []
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                point_id = str(uuid.uuid4())  # Generate a unique ID
                
                # Create metadata with the required structure
                metadata = {
                    "text": chunk["text"],
                    "metadata": chunk["metadata"]
                }
                
                # Create the point
                point = PointStruct(
                    id=point_id,
                    vector=embedding,
                    payload=metadata
                )
                points.append(point)
            
            # Upload points to Qdrant in batches
            batch_size = 100  # Adjust based on your needs
            for i in range(0, len(points), batch_size):
                batch = points[i:i + batch_size]
                self.qdrant_client.upsert(
                    collection_name=COLLECTION_NAME,
                    points=batch
                )
                logger.info(f"Uploaded batch {i//batch_size + 1}/{(len(points)-1)//batch_size + 1} to Qdrant")
            
            return True
            
        except Exception as e:
            logger.error(f"Error uploading to Qdrant: {str(e)}")
            return False

    def process_pdf(self, pdf_path: str) -> bool:
        """Process a single PDF: extract text, chunk, embed, and upload to Qdrant."""
        try:
            # Extract text from PDF
            pages = self.extract_text_from_pdf(pdf_path)
            if not pages:
                logger.warning(f"No text extracted from {pdf_path}")
                return False
            
            # Chunk the text
            chunks = self.chunk_text(pages)
            if not chunks:
                logger.warning(f"No chunks created from {pdf_path}")
                return False
            
            # Get the text from each chunk for embedding
            texts = [chunk["text"] for chunk in chunks]
            
            # Generate embeddings in batches to avoid memory issues
            batch_size = 32  # Adjust based on your embedding model's capacity
            all_embeddings = []
            
            for i in tqdm(range(0, len(texts), batch_size), desc="Generating embeddings"):
                batch_texts = texts[i:i + batch_size]
                batch_embeddings = self.get_embeddings(batch_texts)
                all_embeddings.extend(batch_embeddings)
                
                # Small delay to prevent overloading the API
                time.sleep(0.1)
            
            # Upload chunks and embeddings to Qdrant
            success = self.upload_to_qdrant(chunks, all_embeddings)
            
            return success
            
        except Exception as e:
            logger.error(f"Error processing PDF {pdf_path}: {str(e)}")
            return False

def ensure_collection_exists(client: QdrantClient, collection_name: str) -> None:
    """Ensure the Qdrant collection exists, create it if it doesn't."""
    try:
        collections = client.get_collections().collections
        collection_names = [collection.name for collection in collections]
        
        if collection_name not in collection_names:
            logger.info(f"Creating collection '{collection_name}'...")
            client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=EMBEDDING_DIMENSION,
                    distance=Distance.COSINE
                )
            )
            logger.info(f"Collection '{collection_name}' created successfully")
        else:
            logger.info(f"Collection '{collection_name}' already exists")
            
    except Exception as e:
        logger.error(f"Error ensuring collection exists: {str(e)}")
        raise

def main():
    # Get user input instead of command-line arguments
    print("PDF Processing and Embedding Tool")
    print("=================================")
    
    pdf_dir = input("Enter the directory containing PDFs: ")
    embedding_url = input(f"Enter embedding API URL (default: {EMBEDDING_API_URL}): ") or EMBEDDING_API_URL
    qdrant_host = input(f"Enter Qdrant host (default: {QDRANT_HOST}): ") or QDRANT_HOST
    qdrant_port = input(f"Enter Qdrant port (default: {QDRANT_PORT}): ") or QDRANT_PORT
    collection = input(f"Enter Qdrant collection name (default: {COLLECTION_NAME}): ") or COLLECTION_NAME
    
    # Try to convert port to int
    try:
        qdrant_port = int(qdrant_port)
    except ValueError:
        logger.error(f"Invalid port number: {qdrant_port}")
        sys.exit(1)
    
    # Initialize Qdrant client
    try:
        qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
        logger.info(f"Connected to Qdrant at {qdrant_host}:{qdrant_port}")
    except Exception as e:
        logger.error(f"Failed to connect to Qdrant: {str(e)}")
        sys.exit(1)
    
    # Ensure collection exists
    ensure_collection_exists(qdrant_client, collection)
    
    # Initialize PDF processor
    pdf_processor = PDFProcessor(embedding_url, qdrant_client)
    
    # Process all PDFs in the directory
    pdf_dir_path = Path(pdf_dir)
    
    # Check if the provided path is a directory or a file
    if pdf_dir_path.is_file() and pdf_dir_path.suffix.lower() == '.pdf':
        # Process single PDF file
        logger.info(f"Processing single PDF file: {pdf_dir_path.name}")
        success = pdf_processor.process_pdf(str(pdf_dir_path))
        if success:
            logger.info(f"Successfully processed {pdf_dir_path.name}")
        else:
            logger.error(f"Failed to process {pdf_dir_path.name}")
    else:
        # Process all PDFs in the directory
        pdf_files = list(pdf_dir_path.glob("*.pdf"))
        
        if not pdf_files:
            logger.warning(f"No PDF files found in {pdf_dir}")
            sys.exit(0)
        
        logger.info(f"Found {len(pdf_files)} PDF files to process")
        
        # Process each PDF
        for pdf_file in pdf_files:
            logger.info(f"Processing {pdf_file.name}...")
            success = pdf_processor.process_pdf(str(pdf_file))
            if success:
                logger.info(f"Successfully processed {pdf_file.name}")
            else:
                logger.error(f"Failed to process {pdf_file.name}")
    
    logger.info("Processing complete!")

if __name__ == "__main__":
    main()
