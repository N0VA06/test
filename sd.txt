# import requests
# import json
# import os
# import PyPDF2
# import uuid
# from typing import List, Dict, Any, Optional
# from qdrant_client import QdrantClient
# from qdrant_client.http import models
# import time

# # Configuration with RTX 4060 optimized defaults
# EMBEDDING_API_URL = "http://127.0.0.1:8080/embed"
# QDRANT_HOST = "localhost"
# QDRANT_PORT = 6333
# COLLECTION_NAME = "pdf_embeddings"
# VECTOR_SIZE = 768  # BGE-large model has 768 dimensions
# CHUNK_SIZE = 1000  # Smaller chunk size optimized for RTX 4060
# RETRY_COUNT = 3
# RETRY_DELAY = 2
# # Much smaller payload size for RTX 4060
# MAX_PAYLOAD_SIZE = 800000  # 800KB instead of the previous 1.9MB
# MAX_BATCH_TOKENS = 8192  # Reduced from 16384
# MAX_TEXT_STORAGE = 5000  # Reduced from 10000

# def create_collection_if_not_exists(client: QdrantClient, collection_name: str, vector_size: int):
#     """Create a collection in Qdrant if it doesn't already exist."""
#     collections = client.get_collections().collections
#     if not any(collection.name == collection_name for collection in collections):
#         client.create_collection(
#             collection_name=collection_name,
#             vectors_config=models.VectorParams(
#                 size=vector_size,
#                 distance=models.Distance.COSINE
#             )
#         )
#         print(f"Collection '{collection_name}' created.")
#     else:
#         print(f"Collection '{collection_name}' already exists.")

# def extract_text_from_pdf(pdf_path: str) -> List[Dict[str, Any]]:
#     """
#     Extract text from PDF file page by page with metadata.
#     Returns a list of dictionaries, each containing page text and metadata.
#     """
#     result = []
    
#     try:
#         with open(pdf_path, 'rb') as file:
#             pdf_reader = PyPDF2.PdfReader(file)
#             total_pages = len(pdf_reader.pages)
            
#             # Extract filename without extension as author (modify as needed)
#             author = os.path.splitext(os.path.basename(pdf_path))[0]
#             filename = os.path.basename(pdf_path)
            
#             for page_num in range(total_pages):
#                 page = pdf_reader.pages[page_num]
#                 text = page.extract_text()
                
#                 if text.strip():  # Only process non-empty pages
#                     page_data = {
#                         "text": text,
#                         "metadata": {
#                             "page": page_num + 1,
#                             "total_pages": total_pages,
#                             "source": pdf_path,
#                             "filename": filename
#                         }
#                     }
#                     result.append(page_data)
#     except Exception as e:
#         print(f"Error reading PDF {pdf_path}: {e}")
    
#     return result

# def chunk_text(text: str, max_chunk_size: int) -> List[str]:
#     """Split text into smaller chunks optimized for RTX 4060."""
#     chunks = []
    
#     # If text is already small enough, return it as a single chunk
#     if len(text) <= max_chunk_size:
#         return [text]
    
#     # For very large text, just create chunks of fixed size
#     # This is more memory-efficient than complex splitting
#     if len(text) > 10 * max_chunk_size:
#         # Simple chunking by fixed size for very large text
#         start = 0
#         while start < len(text):
#             end = min(start + max_chunk_size, len(text))
#             # Try to end at a sentence or paragraph boundary if possible
#             if end < len(text):
#                 # Look for paragraph breaks first
#                 para_end = text.rfind('\n\n', start, end)
#                 if para_end > start + (max_chunk_size // 2):
#                     end = para_end + 2
#                 else:
#                     # Look for sentence breaks
#                     sent_end = text.rfind('. ', start, end)
#                     if sent_end > start + (max_chunk_size // 2):
#                         end = sent_end + 2
            
#             chunks.append(text[start:end])
#             start = end
#         return chunks
    
#     # For moderately sized text, use a more semantically aware approach
#     # Split by paragraphs first
#     paragraphs = text.split("\n\n")
    
#     current_chunk = ""
#     for paragraph in paragraphs:
#         # If a single paragraph is too large, split it into sentences
#         if len(paragraph) > max_chunk_size:
#             # Add any accumulated chunk before processing the large paragraph
#             if current_chunk:
#                 chunks.append(current_chunk)
#                 current_chunk = ""
                
#             # Split large paragraph into sentences
#             sentences = paragraph.replace("\n", " ").split(". ")
#             sub_chunk = ""
            
#             for sentence in sentences:
#                 # Add period back if it was removed during splitting
#                 if sentence and not sentence.endswith("."):
#                     sentence += "."
                    
#                 # If adding this sentence would exceed max size, start a new chunk
#                 if len(sub_chunk) + len(sentence) + 1 > max_chunk_size:
#                     if sub_chunk:  # Only append non-empty chunks
#                         chunks.append(sub_chunk)
                    
#                     # If this sentence itself is too large, split it
#                     if len(sentence) > max_chunk_size:
#                         # Split the sentence into smaller parts
#                         words = sentence.split()
#                         word_chunk = ""
#                         for word in words:
#                             if len(word_chunk) + len(word) + 1 > max_chunk_size:
#                                 chunks.append(word_chunk)
#                                 word_chunk = word
#                             else:
#                                 if word_chunk:
#                                     word_chunk += " " + word
#                                 else:
#                                     word_chunk = word
#                         if word_chunk:
#                             sub_chunk = word_chunk
#                         else:
#                             sub_chunk = ""
#                     else:
#                         sub_chunk = sentence
#                 else:
#                     # Add space before adding a new sentence if chunk isn't empty
#                     if sub_chunk:
#                         sub_chunk += " " + sentence
#                     else:
#                         sub_chunk = sentence
            
#             # Add the last sub-chunk if it's not empty
#             if sub_chunk:
#                 chunks.append(sub_chunk)
#         else:
#             # If adding this paragraph would exceed max size, start a new chunk
#             if len(current_chunk) + len(paragraph) + 2 > max_chunk_size:
#                 if current_chunk:  # Only append non-empty chunks
#                     chunks.append(current_chunk)
#                 current_chunk = paragraph
#             else:
#                 # Add newlines before adding a new paragraph if chunk isn't empty
#                 if current_chunk:
#                     current_chunk += "\n\n" + paragraph
#                 else:
#                     current_chunk = paragraph
    
#     # Add the last chunk if it's not empty
#     if current_chunk:
#         chunks.append(current_chunk)
    
#     return chunks

# def get_embedding_with_retry(text: str, max_retries: int = RETRY_COUNT) -> Optional[List[float]]:
#     """Get embedding vector for text with retry mechanism and aggressive size reduction."""
#     retries = 0
#     current_text = text
    
#     while retries <= max_retries:
#         try:
#             # Don't send empty text
#             if not current_text.strip():
#                 print("Warning: Empty text provided to embedding service")
#                 return None
            
#             # First, make sure text is under chunk size
#             if len(current_text) > CHUNK_SIZE:
#                 print(f"Text too long ({len(current_text)} chars), truncating to {CHUNK_SIZE} chars")
#                 current_text = current_text[:CHUNK_SIZE]
                
#             # Create payload and check its size
#             payload = {"inputs": current_text}
#             payload_json = json.dumps(payload)
#             payload_size = len(payload_json.encode('utf-8'))
            
#             # If payload is still too large, reduce further
#             if payload_size > MAX_PAYLOAD_SIZE:
#                 reduction_factor = MAX_PAYLOAD_SIZE / payload_size * 0.8  # More aggressive 80% of limit
#                 new_size = int(len(current_text) * reduction_factor)
#                 print(f"Warning: Payload size ({payload_size} bytes) exceeds limit ({MAX_PAYLOAD_SIZE} bytes)")
#                 print(f"Reducing text from {len(current_text)} to {new_size} characters")
#                 current_text = current_text[:new_size]
#                 payload = {"inputs": current_text}
            
#             headers = {"Content-Type": "application/json"}
            
#             response = requests.post(EMBEDDING_API_URL, data=json.dumps(payload), headers=headers)
#             response.raise_for_status()
            
#             # Parse the response
#             response_data = response.json()
            
#             # Handle different embedding formats
#             if isinstance(response_data, list) and len(response_data) == 1 and isinstance(response_data[0], list):
#                 return response_data[0]
                
#             if isinstance(response_data, list) and all(isinstance(x, (int, float)) for x in response_data):
#                 return response_data
                
#             if isinstance(response_data, dict):
#                 if 'embeddings' in response_data:
#                     embeddings = response_data['embeddings']
#                     if isinstance(embeddings, list):
#                         if len(embeddings) > 0:
#                             if isinstance(embeddings[0], list):
#                                 return embeddings[0]
#                             return embeddings
                
#                 # Try to find the first list of floats in any key
#                 for key, value in response_data.items():
#                     if isinstance(value, list) and len(value) > 0:
#                         if all(isinstance(x, (int, float)) for x in value):
#                             return value
#                         if isinstance(value[0], list) and all(isinstance(x, (int, float)) for x in value[0]):
#                             return value[0]
            
#             print(f"Unrecognized embedding format: {str(response_data)[:100]}...")
#             return None
            
#         except requests.exceptions.HTTPError as e:
#             # Handle 413 Payload Too Large specifically
#             if e.response.status_code == 413:
#                 # Reduce chunk size more aggressively for 413 errors
#                 new_size = len(current_text) // 3  # More aggressive reduction (1/3 instead of 1/2)
#                 if new_size < 300:  # Lower minimum size for RTX 4060
#                     print(f"Error: Payload still too large even with minimal text. API may have strict limits.")
#                     return None
                
#                 print(f"413 Payload Too Large error. Retrying with {new_size} characters")
#                 current_text = current_text[:new_size]
#                 retries += 1
#                 time.sleep(RETRY_DELAY)
#             else:
#                 print(f"HTTP error: {e}")
#                 retries += 1
#                 time.sleep(RETRY_DELAY)
                
#         except Exception as e:
#             print(f"Error getting embedding (attempt {retries+1}/{max_retries+1}): {e}")
#             retries += 1
#             time.sleep(RETRY_DELAY)
    
#     print("Failed to get embedding after maximum retries")
#     return None

# def average_embeddings(embeddings: List[List[float]]) -> List[float]:
#     """Average multiple embeddings into a single embedding vector."""
#     if not embeddings:
#         return None
    
#     # Sum all embeddings
#     avg_embedding = [0.0] * len(embeddings[0])
#     for embedding in embeddings:
#         for i in range(len(embedding)):
#             avg_embedding[i] += embedding[i]
    
#     # Divide by count to get average
#     for i in range(len(avg_embedding)):
#         avg_embedding[i] /= len(embeddings)
    
#     return avg_embedding

# def process_page_with_chunking(text: str, metadata: Dict[str, Any]) -> Optional[List[float]]:
#     """Process page text by chunking for RTX 4060."""
#     # Skip empty text
#     if not text.strip():
#         return None
    
#     # For RTX 4060, always chunk to keep memory usage low
#     chunks = chunk_text(text, CHUNK_SIZE)
#     print(f"Page {metadata['page']} split into {len(chunks)} chunks")
    
#     # For single chunk pages, try direct embedding
#     if len(chunks) == 1:
#         embedding = get_embedding_with_retry(chunks[0])
#         if embedding:
#             return embedding
    
#     # For multi-chunk pages, process in small batches to avoid memory issues
#     chunk_embeddings = []
#     batch_size = 5  # Process 5 chunks at a time to avoid memory pressure
    
#     for i in range(0, len(chunks), batch_size):
#         batch = chunks[i:i+batch_size]
#         print(f"Processing batch {i//batch_size + 1}/{(len(chunks) + batch_size - 1)//batch_size}")
        
#         for j, chunk in enumerate(batch):
#             chunk_index = i + j
#             print(f"Processing chunk {chunk_index+1}/{len(chunks)} for page {metadata['page']} ({len(chunk)} chars)")
#             embedding = get_embedding_with_retry(chunk)
            
#             if embedding:
#                 chunk_embeddings.append(embedding)
#             else:
#                 print(f"Warning: Failed to get embedding for chunk {chunk_index+1} of page {metadata['page']}")
                
#         # Give the GPU a moment to cool down between batches
#         if i + batch_size < len(chunks):
#             print("Pausing briefly between batches...")
#             time.sleep(1)
    
#     if chunk_embeddings:
#         print(f"Successfully embedded {len(chunk_embeddings)}/{len(chunks)} chunks for page {metadata['page']}")
#         return average_embeddings(chunk_embeddings)
#     else:
#         return None

# def process_pdf_to_qdrant(pdf_path: str, client: QdrantClient, collection_name: str):
#     """Process a PDF file and store its embeddings in Qdrant."""
#     pages_data = extract_text_from_pdf(pdf_path)
    
#     if not pages_data:
#         print(f"No text extracted from {pdf_path}")
#         return
    
#     print(f"Processing {len(pages_data)} pages from {pdf_path}")
#     success_count = 0
    
#     for page_data in pages_data:
#         text = page_data["text"]
#         metadata = page_data["metadata"]
        
#         print(f"Processing page {metadata['page']} of {metadata['total_pages']} from {pdf_path}")
        
#         # Process page with chunking strategy
#         embedding = process_page_with_chunking(text, metadata)
        
#         if embedding:
#             # Generate a unique ID for this embedding
#             point_id = str(uuid.uuid4())
            
#             # Store in Qdrant with simplified payload structure
#             try:
#                 # Create payload with all metadata inside a single metadata tag
#                 payload = {
#                     "metadata": metadata,  # This already contains page, total_pages, source, filename
#                     "text_length": len(text)
#                 }
                
#                 # Store full or truncated text
#                 if len(text) <= MAX_TEXT_STORAGE:
#                     payload["text"] = text
#                 else:
#                     # Store beginning and end of text with shorter segments
#                     third_size = MAX_TEXT_STORAGE // 3
#                     payload["text"] = text[:third_size] + " [...] " + text[-third_size:]
#                     payload["metadata"]["text_truncated"] = True
                
#                 client.upsert(
#                     collection_name=collection_name,
#                     points=[
#                         models.PointStruct(
#                             id=point_id,
#                             vector=embedding,
#                             payload=payload
#                         )
#                     ]
#                 )
#                 print(f"Successfully added embedding for page {metadata['page']} to Qdrant")
#                 success_count += 1
#             except Exception as e:
#                 print(f"Error storing embedding in Qdrant: {e}")
#         else:
#             print(f"Failed to get embedding for page {metadata['page']}")
    
#     print(f"Successfully processed {success_count}/{len(pages_data)} pages from {pdf_path}")

# def main():
#     print("\nPDF Embedding to Qdrant (RTX 4060 Optimized)")
#     print("===========================================")
#     print(f"Embedding service: {EMBEDDING_API_URL}")
#     print(f"Qdrant: {QDRANT_HOST}:{QDRANT_PORT}")
#     print(f"Collection: {COLLECTION_NAME}")
#     print(f"Vector size: {VECTOR_SIZE}")
#     print(f"Chunk size: {CHUNK_SIZE} chars")
#     print(f"Max payload size: {MAX_PAYLOAD_SIZE} bytes")
    
#     print("\nNote: For best results with RTX 4060, consider running text-embeddings-router with:")
#     print(f"  --payload-limit 800000 --max-batch-tokens 8192 --max-client-batch-size 8")
#     print("===========================================\n")
    
#     # Get PDF paths from user input
#     print("Enter PDF file paths (one per line, enter a blank line to finish):")
#     pdf_files = []
#     while True:
#         pdf_path = input().strip()
#         if not pdf_path:
#             break
#         pdf_files.append(pdf_path)
    
#     if not pdf_files:
#         print("No PDF files specified. Exiting.")
#         return
    
#     # Initialize Qdrant client
#     client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
    
#     # Create collection if it doesn't exist
#     create_collection_if_not_exists(client, COLLECTION_NAME, VECTOR_SIZE)
    
#     # Process each PDF
#     for pdf_path in pdf_files:
#         if os.path.exists(pdf_path):
#             process_pdf_to_qdrant(pdf_path, client, COLLECTION_NAME)
#         else:
#             print(f"File not found: {pdf_path}")
    
#     print("\nProcessing complete!")

# if __name__ == "__main__":
#     main()
import requests
import json
import os
import PyPDF2
import uuid
from typing import List, Dict, Any, Optional
from qdrant_client import QdrantClient
from qdrant_client.http import models
import time
import numpy as np

# Configuration optimized for quality (no GPU constraints)
EMBEDDING_API_URL = "http://127.0.0.1:8080/embed"
QDRANT_HOST = "localhost"
QDRANT_PORT = 6333
COLLECTION_NAME = "pdf_embeddings"
VECTOR_SIZE = 768  # BGE-large model has 768 dimensions

# Quality-focused parameters
CHUNK_SIZE = 2000  # Larger chunks for better context
CHUNK_OVERLAP = 500  # Overlap between chunks to maintain context
MAX_PAYLOAD_SIZE = 39000  # Much larger payload size (3.9MB)
RETRY_COUNT = 3
RETRY_DELAY = 1
MAX_TEXT_STORAGE = 3000  # Store more text in Qdrant

# Embedding quality strategy (options: "average", "max_pool", "weighted")
EMBEDDING_STRATEGY = "weighted"

def create_collection_if_not_exists(client: QdrantClient, collection_name: str, vector_size: int):
    """Create a collection in Qdrant if it doesn't already exist."""
    collections = client.get_collections().collections
    if not any(collection.name == collection_name for collection in collections):
        client.create_collection(
            collection_name=collection_name,
            vectors_config=models.VectorParams(
                size=vector_size,
                distance=models.Distance.COSINE
            )
        )
        print(f"Collection '{collection_name}' created.")
    else:
        print(f"Collection '{collection_name}' already exists.")

def extract_text_from_pdf(pdf_path: str) -> List[Dict[str, Any]]:
    """
    Extract text from PDF file page by page with metadata.
    Returns a list of dictionaries, each containing page text and metadata.
    """
    result = []
    
    try:
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)
            
            # Extract filename without extension as author (modify as needed)
            author = os.path.splitext(os.path.basename(pdf_path))[0]
            filename = os.path.basename(pdf_path)
            
            for page_num in range(total_pages):
                page = pdf_reader.pages[page_num]
                text = page.extract_text()
                
                if text.strip():  # Only process non-empty pages
                    page_data = {
                        "text": text,
                        "metadata": {
                            "page": page_num + 1,
                            "total_pages": total_pages,
                            "source": pdf_path,
                            "filename": filename
                        }
                    }
                    result.append(page_data)
    except Exception as e:
        print(f"Error reading PDF {pdf_path}: {e}")
    
    return result

def chunk_text_with_overlap(text: str, chunk_size: int, overlap: int) -> List[str]:
    """Split text into chunks with overlap for better context preservation."""
    chunks = []
    
    # If text is already small enough, return it as a single chunk
    if len(text) <= chunk_size:
        return [text]
    
    # Create overlapping chunks
    start = 0
    while start < len(text):
        # Calculate end position with overlap
        end = min(start + chunk_size, len(text))
        
        # Try to end at a natural boundary if possible
        if end < len(text):
            # Look for paragraph breaks first
            para_end = text.rfind('\n\n', start, end)
            if para_end > start + (chunk_size // 2):
                end = para_end + 2
            else:
                # Look for sentence breaks
                sent_end = text.rfind('. ', start, end)
                if sent_end > start + (chunk_size // 2):
                    end = sent_end + 2
        
        # Add this chunk
        chunks.append(text[start:end])
        
        # Move start position accounting for overlap
        start = end - overlap if end < len(text) else len(text)
        
        # Avoid creating tiny chunks at the end
        if len(text) - start < overlap:
            break
    
    # Add the final piece if there's text remaining
    if start < len(text):
        chunks.append(text[start:])
    
    return chunks

def get_embedding(text: str) -> Optional[List[float]]:
    """Get embedding vector for text without retries - for better quality version."""
    try:
        # Don't send empty text
        if not text.strip():
            print("Warning: Empty text provided to embedding service")
            return None
                
        # Create payload
        payload = {"inputs": text}
        headers = {"Content-Type": "application/json"}
        
        response = requests.post(EMBEDDING_API_URL, data=json.dumps(payload), headers=headers)
        response.raise_for_status()
        
        # Parse the response
        response_data = response.json()
        
        # Handle different embedding formats
        if isinstance(response_data, list) and len(response_data) == 1 and isinstance(response_data[0], list):
            return response_data[0]
            
        if isinstance(response_data, list) and all(isinstance(x, (int, float)) for x in response_data):
            return response_data
            
        if isinstance(response_data, dict):
            if 'embeddings' in response_data:
                embeddings = response_data['embeddings']
                if isinstance(embeddings, list):
                    if len(embeddings) > 0:
                        if isinstance(embeddings[0], list):
                            return embeddings[0]
                        return embeddings
            
            # Try to find the first list of floats in any key
            for key, value in response_data.items():
                if isinstance(value, list) and len(value) > 0:
                    if all(isinstance(x, (int, float)) for x in value):
                        return value
                    if isinstance(value[0], list) and all(isinstance(x, (int, float)) for x in value[0]):
                        return value[0]
        
        print(f"Unrecognized embedding format: {str(response_data)[:100]}...")
        return None
        
    except requests.exceptions.HTTPError as e:
        # Handle 413 Payload Too Large specifically
        if e.response.status_code == 413:
            print(f"413 Payload Too Large error. Consider increasing max payload size for embedding API.")
            return None
        else:
            print(f"HTTP error: {e}")
            return None
            
    except Exception as e:
        print(f"Error getting embedding: {e}")
        return None

def calculate_text_stats(text: str) -> Dict[str, float]:
    """Calculate statistics about the text to determine its information density."""
    stats = {}
    
    # Simple measures of information density
    stats["length"] = len(text)
    stats["unique_words"] = len(set(text.lower().split()))
    
    if len(text.split()) > 0:
        stats["unique_ratio"] = stats["unique_words"] / len(text.split())
    else:
        stats["unique_ratio"] = 0
    
    # Check for presence of numerical data (often important)
    stats["has_numbers"] = 1.0 if any(c.isdigit() for c in text) else 0.0
    
    # Presence of special characters (might indicate diagrams, tables, etc.)
    special_chars = set("@#$%^&*()[]{}<>|~`+=/\\")
    stats["special_chars_ratio"] = len([c for c in text if c in special_chars]) / max(1, len(text))
    
    return stats

def average_embeddings(embeddings: List[List[float]]) -> List[float]:
    """Simple average of embeddings."""
    if not embeddings:
        return None
    
    # Sum all embeddings
    avg_embedding = [0.0] * len(embeddings[0])
    for embedding in embeddings:
        for i in range(len(embedding)):
            avg_embedding[i] += embedding[i]
    
    # Divide by count to get average
    for i in range(len(avg_embedding)):
        avg_embedding[i] /= len(embeddings)
    
    return avg_embedding

def max_pool_embeddings(embeddings: List[List[float]]) -> List[float]:
    """Use max pooling on embeddings - tends to capture the most distinctive features."""
    if not embeddings:
        return None
    
    # Initialize with the first embedding
    max_vals = list(embeddings[0])
    
    # Find element-wise maximum
    for embedding in embeddings[1:]:
        for i in range(len(embedding)):
            max_vals[i] = max(max_vals[i], embedding[i])
    
    return max_vals

def weighted_average_embeddings(embeddings: List[List[float]], weights: List[float]) -> List[float]:
    """Weighted average of embeddings based on importance weights."""
    if not embeddings:
        return None
    
    # Normalize weights
    total_weight = sum(weights)
    if total_weight == 0:
        # If all weights are 0, use simple average
        return average_embeddings(embeddings)
    
    norm_weights = [w / total_weight for w in weights]
    
    # Initialize result
    result = [0.0] * len(embeddings[0])
    
    # Calculate weighted sum
    for i, embedding in enumerate(embeddings):
        weight = norm_weights[i]
        for j, value in enumerate(embedding):
            result[j] += value * weight
    
    return result

def combine_embeddings(embeddings: List[List[float]], texts: List[str], strategy: str = "average") -> List[float]:
    """Combine multiple embeddings using the specified strategy."""
    if not embeddings:
        return None
    
    if len(embeddings) == 1:
        return embeddings[0]
    
    if strategy == "average":
        return average_embeddings(embeddings)
    
    elif strategy == "max_pool":
        return max_pool_embeddings(embeddings)
    
    elif strategy == "weighted":
        # Calculate weights based on text statistics
        weights = []
        for text in texts:
            stats = calculate_text_stats(text)
            # Create a composite importance score (can be adjusted based on domain)
            importance = (
                stats["length"] * 0.3 +  # Longer chunks might have more info
                stats["unique_ratio"] * 10.0 +  # Higher lexical diversity suggests more information
                stats["has_numbers"] * 2.0 +  # Numbers often indicate important facts
                stats["special_chars_ratio"] * 5.0  # Special characters might indicate tables or diagrams
            )
            weights.append(importance)
        
        return weighted_average_embeddings(embeddings, weights)
    
    else:
        # Default to average if strategy not recognized
        print(f"Warning: Unknown embedding strategy '{strategy}', using average instead")
        return average_embeddings(embeddings)

def process_page_high_quality(text: str, metadata: Dict[str, Any]) -> Optional[List[float]]:
    """Process page text with high-quality embedding strategy."""
    # Skip empty text
    if not text.strip():
        return None
    
    # If text is short enough for direct embedding, try that first
    if len(text) <= CHUNK_SIZE:
        embedding = get_embedding(text)
        if embedding:
            return embedding
    
    # For longer texts, use overlapping chunks
    chunks = chunk_text_with_overlap(text, CHUNK_SIZE, CHUNK_OVERLAP)
    print(f"Page {metadata['page']} split into {len(chunks)} chunks with {CHUNK_OVERLAP} char overlap")
    
    # Get embeddings for each chunk
    chunk_embeddings = []
    chunk_texts = []  # Keep track of the texts for weighted strategies
    
    for i, chunk in enumerate(chunks):
        print(f"Processing chunk {i+1}/{len(chunks)} for page {metadata['page']} ({len(chunk)} chars)")
        embedding = get_embedding(chunk)
        
        if embedding:
            chunk_embeddings.append(embedding)
            chunk_texts.append(chunk)
        else:
            print(f"Warning: Failed to get embedding for chunk {i+1} of page {metadata['page']}")
    
    if chunk_embeddings:
        print(f"Successfully embedded {len(chunk_embeddings)}/{len(chunks)} chunks for page {metadata['page']}")
        # Combine embeddings using the selected strategy
        return combine_embeddings(chunk_embeddings, chunk_texts, EMBEDDING_STRATEGY)
    else:
        return None

def normalize_embedding(embedding: List[float]) -> List[float]:
    """Normalize the embedding vector to unit length for cosine similarity."""
    # Convert to numpy array for easier math
    vec = np.array(embedding)
    # Calculate the L2 norm (Euclidean length)
    norm = np.linalg.norm(vec)
    # Normalize and convert back to list
    if norm > 0:
        return (vec / norm).tolist()
    return embedding

def process_pdf_to_qdrant(pdf_path: str, client: QdrantClient, collection_name: str):
    """Process a PDF file and store its embeddings in Qdrant."""
    pages_data = extract_text_from_pdf(pdf_path)
    
    if not pages_data:
        print(f"No text extracted from {pdf_path}")
        return
    
    print(f"Processing {len(pages_data)} pages from {pdf_path}")
    success_count = 0
    
    for page_data in pages_data:
        text = page_data["text"]
        metadata = page_data["metadata"]
        
        print(f"Processing page {metadata['page']} of {metadata['total_pages']} from {pdf_path}")
        
        # Process page with high-quality embedding
        embedding = process_page_high_quality(text, metadata)
        
        if embedding:
            # Normalize embedding for better search results
            embedding = normalize_embedding(embedding)
            
            # Generate a unique ID for this embedding
            point_id = str(uuid.uuid4())
            
            # Store in Qdrant with metadata tag
            try:
                # Create payload
                payload = {
                    "metadata": metadata,
                    "text_length": len(text)
                }
                
                # Store full or truncated text
                if len(text) <= MAX_TEXT_STORAGE:
                    payload["text"] = text
                else:
                    # Store beginning and end of text
                    half_size = MAX_TEXT_STORAGE // 2
                    payload["text"] = text[:half_size] + " [...] " + text[-half_size:]
                    payload["metadata"]["text_truncated"] = True
                
                client.upsert(
                    collection_name=collection_name,
                    points=[
                        models.PointStruct(
                            id=point_id,
                            vector=embedding,
                            payload=payload
                        )
                    ]
                )
                print(f"Successfully added embedding for page {metadata['page']} to Qdrant")
                success_count += 1
            except Exception as e:
                print(f"Error storing embedding in Qdrant: {e}")
        else:
            print(f"Failed to get embedding for page {metadata['page']}")
    
    print(f"Successfully processed {success_count}/{len(pages_data)} pages from {pdf_path}")

def main():
    print("\nPDF Embedding to Qdrant (High Quality Version)")
    print("=============================================")
    print(f"Embedding service: {EMBEDDING_API_URL}")
    print(f"Qdrant: {QDRANT_HOST}:{QDRANT_PORT}")
    print(f"Collection: {COLLECTION_NAME}")
    print(f"Vector size: {VECTOR_SIZE}")
    print(f"Chunk size: {CHUNK_SIZE} chars with {CHUNK_OVERLAP} char overlap")
    print(f"Embedding strategy: {EMBEDDING_STRATEGY}")
    print("=============================================\n")
    
    # Get PDF paths from user input
    print("Enter PDF file paths (one per line, enter a blank line to finish):")
    pdf_files = []
    while True:
        pdf_path = input().strip()
        if not pdf_path:
            break
        pdf_files.append(pdf_path)
    
    if not pdf_files:
        print("No PDF files specified. Exiting.")
        return
    
    # Initialize Qdrant client
    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
    
    # Create collection if it doesn't exist
    create_collection_if_not_exists(client, COLLECTION_NAME, VECTOR_SIZE)
    
    # Process each PDF
    for pdf_path in pdf_files:
        if os.path.exists(pdf_path):
            process_pdf_to_qdrant(pdf_path, client, COLLECTION_NAME)
        else:
            print(f"File not found: {pdf_path}")
    
    print("\nProcessing complete!")

if __name__ == "__main__":
    main()
